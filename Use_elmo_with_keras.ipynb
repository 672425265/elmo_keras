{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Use elmo with keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ItXfxkxvosLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Using ELMO in keras models\n",
        "## Task: Sentiment analysis of movie reviews"
      ]
    },
    {
      "metadata": {
        "id": "2ew7HTbPpCJH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, CuDNNLSTM\n",
        "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Input, Lambda\n",
        "from tensorflow.keras import Model\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iAsKG535pHep",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the IMDB dataset\n",
        "\n",
        "The IMDB dataset is conviently preprocessed by others and can be easily obtained using Keras. \n",
        "The reviews (sequences of words) have been converted to sequences of integers, where each integer represents a specific word in a dictionary. The dictionary is also pre-built. \n",
        "\n",
        "Keras contains the following helper function that downloads the IMDB dataset to your machine.\n",
        "\n",
        "```python\n",
        "def load_data(path='imdb.npz', num_words=None, skip_top=0, maxlen=None, seed=113,\n",
        "              start_char=1, oov_char=2, index_from=3, **kwargs):\n",
        "```\n",
        "\n",
        "We talked about the design challenge of setting a vocabulary size. For now, we will set it to 10,000 words."
      ]
    },
    {
      "metadata": {
        "id": "zXXx5Oc3pOmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = \\\n",
        "       keras.datasets.imdb.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FPWkGzYL_ro",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Keras also comes with a pre-built dictionary of mapping words to its ID. However, it does not match the preprocessed word IDs. We need to add special words into this dictionary. `<PAD>` `<SOS>` `<UNK>` are added to match the settings `start_char=1, oov_char=2, index_from=3` in `load_data()`.\n",
        "\n",
        "It is common in NLP to add these special words in the dictionary. We want to add the *PADDING* symbol `<PAD>`, *Start-of-sentence* symbol `<SOS>`, and *Unknown* symbol `<UNK>`."
      ]
    },
    {
      "metadata": {
        "id": "BVBVl8g10y0d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A dictionary mapping words --> integer index\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "\n",
        "# Shift word index by 3 because we want to add special words\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} \n",
        "word_index[\"<PAD>\"] = 0  # padding\n",
        "word_index[\"<SOS>\"] = 1  # start of sequence\n",
        "word_index[\"<UNK>\"] = 2  # unknown (out of the top 10,000 most frequent words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VXdoEKwi0y0h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For our convenience, we will create a helper function to convert integer IDs back to words. It is easier to find errors that way!"
      ]
    },
    {
      "metadata": {
        "id": "-c6Anz8R0y0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build another dictionary of mapping integer --> words \n",
        "reverse_word_index = dict([(v, k) for (k, v) in word_index.items()])\n",
        "\n",
        "# Create a helper function to convert the integer to words\n",
        "def decode_review(text):\n",
        "  words = [reverse_word_index[i] for i in text]\n",
        "  return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l50X3GfjpU4r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Always check the content\n",
        "\n",
        "### Confirm the correctness of preprocessing, because things can go wrong in so many ways.\n",
        "\n",
        "When you deal with your own dataset, you have to write your own preprocessing procedure. Remember to check the correctness of the preprocessing!\n",
        "\n",
        "In our design, each input should contain a list of integers representing the words of the movie review, and output should be an integer of 0 or 1. We use 0 to represent a negative review and 1 positive. First we want to make sure the number of reviews and labels are equal."
      ]
    },
    {
      "metadata": {
        "id": "y8qCnve_-lkO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Training data: {} reviews, {} labels\".format(len(train_data), len(train_labels)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RnKvHWW4-lkW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The words should be converted to integers, where each integer represents a specific word in a dictionary. Here's what the first review looks like:"
      ]
    },
    {
      "metadata": {
        "id": "QtTS4kpEpjbi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Word IDs\")\n",
        "print(train_data[0])\n",
        "print(\"Label\")\n",
        "print(train_labels[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hIE4l_72x7DP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Movie reviews may be of different lengths. We can see by examining a few of them. \n",
        "\n",
        "Since inputs must be the same length, we'll need to resolve this later."
      ]
    },
    {
      "metadata": {
        "id": "X-6Ii9Pfx6Nr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U3CNRvEZVppl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can use the `decode_review` function to display the text for the first review, and also check for any error."
      ]
    },
    {
      "metadata": {
        "id": "s_OqxmH6-lkn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(decode_review(train_data[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2cv9iNJH5ATP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we did **not** add special words in word_index, we will see that the reviews don't make any sense when using `decode_review`.  It is important to always check for errors like this!"
      ]
    },
    {
      "metadata": {
        "id": "lFP_XKVRp4_S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare the data for input\n",
        "\n",
        "No need to pad input sentence when using ELMO. **We need to convert word IDs back to words**!\n",
        "\n",
        "Important options:\n",
        "* Max length = 100\n"
      ]
    },
    {
      "metadata": {
        "id": "2jQv-omsHurp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# NO NEED TO PAD !!!!!\n",
        "# train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
        "#                                                         value=word_index[\"<PAD>\"],\n",
        "#                                                         padding='pre',\n",
        "#                                                         truncating='pre',\n",
        "#                                                         maxlen=100)\n",
        "# test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
        "#                                                        value=word_index[\"<PAD>\"],\n",
        "#                                                        padding='pre',\n",
        "#                                                        truncating='pre',\n",
        "#                                                        maxlen=100)\n",
        "maxlen = 100\n",
        "train_data = [decode_review(t[:maxlen]) for t in train_data]\n",
        "test_data = [decode_review(t[:maxlen]) for t in test_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VO5MBpyQdipD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at the length of the examples now:"
      ]
    },
    {
      "metadata": {
        "id": "USSSBnkE-lky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QJoxZGyfjT5V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And inspect the first review:"
      ]
    },
    {
      "metadata": {
        "id": "TG8X9cqi-lk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LTY6qJlZ0y06",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compare the review now vs. the original above."
      ]
    },
    {
      "metadata": {
        "id": "1Az6rbK10y0_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you need to process your own dataset, you can use another convenient helper function in Keras\n",
        "```python\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<UNK>\")\n",
        "# SENTENCES = list of list of words\n",
        "tokenizer.fit_on_texts(SENTENCES)\n",
        "sequences = tokenizer.texts_to_sequences(SENTENCES)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "```\n",
        "\n",
        "Remember to call `pad_sequences` later!"
      ]
    },
    {
      "metadata": {
        "id": "hCWYwkug-llQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a validation set\n",
        "\n",
        "When training, we want to check the accuracy of the model on data it hasn't seen before. Create a *validation set* by setting apart some examples from the original training data. \n",
        "\n",
        "Why not use the testing set now? Our goal is to develop and tune our model using only the training data, then use the test data just once to evaluate our accuracy. \n",
        "\n",
        "To save time, we will use only a part of the training data."
      ]
    },
    {
      "metadata": {
        "id": "-NpcXY9--llS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_val = train_data[:1000]\n",
        "partial_x_train = train_data[1000:4000]\n",
        "\n",
        "y_val = train_labels[:1000]\n",
        "partial_y_train = train_labels[1000:4000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLC02j2g-llC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Keras can help us build a model quickly. The neural network is created by adding layers. However, you need to decide :\n",
        "* How many layers?\n",
        "* How many hidden units to use for each layer?\n",
        "\n",
        "Let's build a simple model for the sentiment analysis problem!"
      ]
    },
    {
      "metadata": {
        "id": "JvdwlhojfOLu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "\n",
        "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.tables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpKOoWgu-llD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We create a function to integrate the tensorflow model with a Keras model\n",
        "# This requires explicitly casting the tensor to a string, because of a Keras quirk\n",
        "def ElmoEmbedding(x):\n",
        "    return elmo_model(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", \\\n",
        "                      as_dict=True)[\"default\"]\n",
        "# the output dictionary:\n",
        "# [\"default\"] -> a fixed mean-pooling of all contextualized word representations\n",
        "#                with shape [batch_size, 1024].\n",
        "# [\"elmo\"]    -> the weighted sum of the 3 layers, where the weights are\n",
        "#                trainable. This tensor has shape [batch_size, max_length, 1024]\n",
        "\n",
        "  \n",
        "input_text = Input(shape=(1,), dtype=tf.string)\n",
        "embedding = Lambda(ElmoEmbedding, output_shape=(1024,))(input_text)\n",
        "dense = Dense(256, activation='relu')(embedding)\n",
        "pred = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=pred)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mr0GP-cQ-llN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "35jv_fzP-llU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Train the model for *n* epochs in mini-batches of samples. Recall that this is *n* iterations over all samples in the training data. While training, monitor the model's loss and accuracy on the validation set:\n",
        "\n",
        "** INPUT SENTENCE MUST BE NUMPY ARRAY! **"
      ]
    },
    {
      "metadata": {
        "id": "tXSGrjWZ-llW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(np.array(partial_x_train),\n",
        "                    partial_y_train,\n",
        "                    epochs=8,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(np.array(x_val), y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9EEGuDVuzb5r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model\n",
        "\n",
        "And let's see how the model performs on the test set. Two values will be returned when calling `.evaluate` function, **loss** (we defined it as binary cross entropy) and **accuracy**.  Keras will report whatever we used in the `model.compile` function as the evaluation metrics.\n"
      ]
    },
    {
      "metadata": {
        "id": "zOMKywn4zReN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = model.evaluate(np.array(test_data), test_labels)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5KggXVeL-llZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a plot of accuracy and loss over time\n",
        "\n",
        "`model.fit()` returns a `History` object that contains a dictionary with everything that happened during training. There are four entries: one for each monitored metric during training and validation. We can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy. We will write a helper function to plot loss and accuracy of each epoch."
      ]
    },
    {
      "metadata": {
        "id": "VcvSXvhp-llb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_hist(history):\n",
        "    history_dict = history.history\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "    # plot for loss\n",
        "    plt.clf()   # clear figure\n",
        "    # \"bo\" is for \"blue dot\"\n",
        "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "    # r is for \"red solid line\"\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # plot for accuracy\n",
        "    plt.clf()   # clear figure\n",
        "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_hist(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_UhWQ00ASV21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a recurrent model\n",
        "\n",
        "You can easily change to a recurrent model in Keras. Simply replace `Dense` with `LSTM` and that is it! \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Specifically, some parameters can be set for the LSTM cell.\n",
        "\n",
        "```python\n",
        "LSTM(hidden_units, dropout=0.0, recurrent_dropout=0.0)\n",
        "```\n",
        "\n",
        "The first `dropout` refers to the dropping of input features, and `recurrent_dropout` refers to the dropping of the previous output. Recall that in the slides we showed some connection between the previous output and the current input?\n",
        "\n",
        "Let's build a recurrent model for the sentiment analysis problem!"
      ]
    },
    {
      "metadata": {
        "id": "WzruqyZT0y1T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = None\n",
        "K.clear_session()\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "\n",
        "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.tables_initializer())\n",
        "\n",
        "# Note the final dictionary key `elmo`\n",
        "def ElmoEmbedding(x):\n",
        "    return elmo_model(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", \\\n",
        "                      as_dict=True)[\"elmo\"]\n",
        "\n",
        "# the output dictionary:\n",
        "# [\"default\"] -> a fixed mean-pooling of all contextualized word representations\n",
        "#                with shape [batch_size, 1024].\n",
        "# [\"elmo\"]    -> the weighted sum of the 3 layers, where the weights are\n",
        "#                trainable. This tensor has shape [batch_size, max_length, 1024]\n",
        "\n",
        "\n",
        "input_text = Input(shape=(1,), dtype=tf.string)\n",
        "embedding = Lambda(ElmoEmbedding, output_shape=(None,1024,))(input_text)\n",
        "rnn_lstm = CuDNNLSTM(16)(embedding)\n",
        "pred = Dense(1, activation='sigmoid')(rnn_lstm)\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=pred)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8wdkMR5X0y1W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.train.AdamOptimizer(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(np.array(partial_x_train), \\\n",
        "                    partial_y_train, \\\n",
        "                    epochs=4, \\\n",
        "                    batch_size=32, \\\n",
        "                    validation_data=(np.array(x_val), y_val))\n",
        "\n",
        "plot_hist(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XU531arM0y1X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "results = model.evaluate(np.array(test_data), test_labels)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gjw9UGT2vgxr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}